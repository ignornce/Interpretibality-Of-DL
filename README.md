### Vision

+ https://github.com/NVlabs/RADIO/tree/main vision 
---
+ https://categoricaldeeplearning.com/ category theory in ml all related resources.
+ category theory intro video  https://www.youtube.com/watch?v=hNRETzXkE5M

LLM tasks taxonomy: https://arxiv.org/pdf/2307.10169.pdf
QFT: resoruce https://www.youtube.com/watch?v=liQoSIaYBJk&list=PLXS_wzQAoIt7GxSRwCaicaSdeKqzYSUq4


## Lottery Ticket Hypothesis Thought Tree
+ Position Paper: Challenges and Opportunities in Topological Deep Learning
+ Q: Is dead neurons related ??
+ Related Main Question why deepLearning works better then wideLearning.
+ https://arxiv.org/pdf/2402.17457.pdf Why do Learning Rates Transfer?
Reconciling Optimization and Scaling Limits for Deep Learning
+ Anythig related ?? what is the winning ticket heissan values during training compared to loser tickets.
Proof + A unified lottery ticket hypothesis for graph neural networks.
----
## Physics Lens
+ Physics-Informed Machine Learning: A Survey
on Problems, Methods and Applications : Survey
+ Equivariant Flows: Exact Likelihood Generative Learning for Symmetric
Densities Particles here!!
+ A mathematical perspective on Transformers. Describes Transformers as Transformers based on their interpretation as interacting particle systems
 ### Quantum Field Theory and NNS
 + Neural Networks and Quantum Field Theory ( Corner stone paper coming with first intuitions).
  +  https://arxiv.org/pdf/2008.08601.pdf
 + Neural Network Field Theories:
Non-Gaussianity, Actions, and Locality : https://arxiv.org/pdf/2307.03223.pdf
+ Symmetry-via-Duality:Invariant Neural Network Densities from Parameter-Space Correlators.
  + Discusses duality and symetry approach in qft. Can be used in NTK.  
+ Random Matrix Theory and Quantum Chromodynamics
+ Asymptotics of Wide Networks
from Feynman Diagrams
+ Building Quantum Field Theories Out of Neurons
   https://arxiv.org/abs/2112.04527

    Gauge equavariant (Flow models) <----Symetry duality  <--QFT == NN
                                               â†“
 Symetry duality ---> lattice  QCD ( Quantum chromodynamics) ---------> Random Matrix Theory ----->> Neural Tangent Kernel 
                                                                                                        
                                               
---
## Random Matrix Theory
+ https://arxiv.org/abs/1812.03403 | Statistical thresholds for Tensor PCA
+ A Random Matrix Perspective on Random Tensors: https://www.jmlr.org/papers/volume23/21-1038/21-1038.pdf
+ MATRICES WITH GAUSSIAN NOISE: OPTIMAL ESTIMATES FOR SINGULAR SUBSPACE PERTURBATION
+ Two-way kernel matrix puncturing:
+  !!! Covariance discriminative power of kernel clustering methods **IMPORTANT FOR UNDERSTANDING SPECTRAL CLUSTERING METHODS**
  + Its to better understand performance of the used kernal using rmt.  
+ PCA-based multi-task learning: a random matrix approach
+ A CONCENTRATION OF MEASURE AND RANDOM MATRIX APPROACH TO LARGE-DIMENSIONAL ROBUST STATISTICS
+ A RANDOM MATRIX APPROACH TO NEURAL NETWORKS
+ Two-way kernel matrix puncturing:
towards resource-efficient PCA and spectral clustering
+ KERNEL SPECTRAL CLUSTERING OF LARGE DIMENSIONAL DATA
  + Its about theorotical base of the kernal spectral clustering.
+ Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars
#### MNIST related in RMT
  + A Large Dimensional Analysis of Least Squares **Support Vector Machines**
  + Random Matrix Improved **Subspace Clustering**
  + Sparse Quantized **Spectral Clustering**
  + **SEMI SUPERVISED LAERNING**  Random Matrix Analysis to Balance between Supervised
and Unsupervised Learning under the Low Density Separation Assumptioncan be used in mnist-1d again
### Understanding NNs via Random matrix theory:
+ DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES
### Theory on Universiality  of GMM
+ Classification of Heavy-tailed Features in High Dimensions: a Superstatistical Approach
#### Footprint improvement 
+ RANDOM MATRICES IN SERVICE OF ML FOOTPRINT:
TERNARY RANDOM FEATURES WITH NO PERFORMANCE LOSS

---- 
Interpretability Claims:
+ https://arxiv.org/abs/2312.01429 Interpretability methods might be misleading.

---

## Information Theory
- Grokking as Compression: A Nonlinear Complexity Perspective  https://arxiv.org/pdf/2310.05918.pdf 
-  In search of dispersed memories: Generative diffusion models are associative memory networks
https://arxiv.org/abs/2309.17290
- White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?
  https://arxiv.org/pdf/2311.13110.pdf
- White-Box Transformers via Sparse Rate Reduction shorter version : https://arxiv.org/pdf/2306.01129.pdf  
---
## Equaivariant NNs
+ https://dmol.pub/dl/Equivariant.html
### Harmonic Cnn like stuff
---
## Category Theory
+ Physical Computing:
A Category Theoretic Perspective on Physical
---
Computation and System Compositionality
## Anthropic Mechanistic Interpretability
+ https://transformer-circuits.pub/2023/monosemantic-features/index.html

## Untagged LLM
+ LANGUAGE MODELS REPRESENT SPACE AND TIME
 https://arxiv.org/pdf/2310.02207.pdf
+ Beyond Surface Statistics : https://arxiv.org/pdf/2306.05720.pdf
+ https://arxiv.org/abs/2308.09124:Linearity of Relation Decoding in Transformer Language Models
+ https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/iGuwZTHWb6DFY3sKB :Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron
+ What does self-attention learn from Masked Language Modelling? | https://arxiv.org/pdf/2304.07235.pdf
+ https://arxiv.org/pdf/2302.10886v3.pdf | SOME INTRIGUING ASPECTS ABOUT LIPSCHITZ CONTINUITY OF NEURAL NETWORKS
## Category Theory
 + https://arxiv.org/pdf/2106.07032.pdf Category Theory in Machine Learning

## In Context Learning Mystery
+ In-Context Learning Creates Task Vectors: https://arxiv.org/pdf/2310.15916.pdf
+ What learning algorithm is in-context learning? Investigations with linear models: https://arxiv.org/abs/2211.15661:


## LLMs doing Math:
+ https://huggingface.co/papers/2309.03241
+ XVAL: A CONTINUOUS NUMBER ENCODING
FOR LARGE LANGUAGE MODELS : https://arxiv.org/abs/2310.02989#:~:text=xVal%3A%20A%20Continuous%20Number%20Encoding%20for%20Large%20Language%20Models,-Siavash%20Golkar%2C%20Mariel&text=We%20propose%20xVal%2C%20a%20numerical,vector%20by%20the%20number%20value.
   
   

## Optimal Transort
+ https://www.youtube.com/watch?v=EauDdCzxphE
+ https://openreview.net/pdf?id=KiespDPaRH
  + IMPROVING THE CONVERGENCE OF DYNAMIC NERFS VIA OPTIMAL TRANSPORT
## Geometry and Topology in Machine Learning
+ https://www.youtube.com/watch?v=sS8zRGgPLNc
+ Meaning comes from geometry most of the time.


## Dead neurons Question/ Mystery
+ https://lena-voita.github.io/posts/neurons_in_llms_dead_ngram_positional.html 
   + https://arxiv.org/pdf/2309.04827.pdf
+ THE LOTTERY TICKET HYPOTHESIS:FINDING SPARSE, TRAINABLE NEURAL NETWORKS https://arxiv.org/pdf/1803.03635.pdf
   + UNIVERSAL NEURONS IN GPT2 LANGUAGE MODELS
   + INTERPRETABILITY ILLUSIONS IN THE  the Generalization of Simplified Models 
## Physic models in DL
+ https://www.youtube.com/watch?v=XLNmgviQHPA LEGO paper
+ 
## Physics Methods in ML
+ # Main ResearchLine Interests
## Transformers are X papers from lens of Category Theory
+ Transformers are Cnn :https://huggingface.co/papers/2401.17574 | Scavenging Hyena: Distilling Transformers into Long Convolution Models | 
+ transformers are rnns papers
+ Transformers are graph attention models| https://thegradient.pub/transformers-are-graph-neural-networks/
+ LLM s are compression https://arxiv.org/pdf/2309.10668.pdf
+  A mathematical perspective on Transformers, Says transformers are flow models.!!! 
