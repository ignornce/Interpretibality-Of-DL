## Random Matrix Theory
+ https://arxiv.org/abs/1812.03403 | Statistical thresholds for Tensor PCA
+ A Random Matrix Perspective on Random Tensors: https://www.jmlr.org/papers/volume23/21-1038/21-1038.pdf
+ MATRICES WITH GAUSSIAN NOISE: OPTIMAL ESTIMATES FOR SINGULAR SUBSPACE PERTURBATION
  + MNIST related
    + A Large Dimensional Analysis of Least Squares **Support Vector Machines**
    + Random Matrix Improved **Subspace Clustering**
    + Sparse Quantized **Spectral Clustering**
    + **SEMI SUPERVISED LAERNING**  Random Matrix Analysis to Balance between Supervised
and Unsupervised Learning under the Low Density Separation Assumptioncan be used in mnist-1d again

+ PCA-based multi-task learning: a random matrix approach
+ A CONCENTRATION OF MEASURE AND RANDOM MATRIX APPROACH TO LARGE-DIMENSIONAL ROBUST STATISTICS
+ A RANDOM MATRIX APPROACH TO NEURAL NETWORKS
## Information Theory
- Grokking as Compression: A Nonlinear Complexity Perspective  https://arxiv.org/pdf/2310.05918.pdf 
-  In search of dispersed memories: Generative diffusion models are associative memory networks
https://arxiv.org/abs/2309.17290
- White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?
  https://arxiv.org/pdf/2311.13110.pdf
- White-Box Transformers via Sparse Rate Reduction shorter version : https://arxiv.org/pdf/2306.01129.pdf  

## Anthropic Mechanistic Interpretability
+ https://transformer-circuits.pub/2023/monosemantic-features/index.html

## Untagged LLM
+ LANGUAGE MODELS REPRESENT SPACE AND TIME
 https://arxiv.org/pdf/2310.02207.pdf
+ Beyond Surface Statistics : https://arxiv.org/pdf/2306.05720.pdf
+ https://arxiv.org/abs/2308.09124:Linearity of Relation Decoding in Transformer Language Models
+ https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/iGuwZTHWb6DFY3sKB :Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron
+ What does self-attention learn from Masked Language Modelling? | https://arxiv.org/pdf/2304.07235.pdf
## Category Theory

## In Context Learning Mystery
+ In-Context Learning Creates Task Vectors: https://arxiv.org/pdf/2310.15916.pdf
+ What learning algorithm is in-context learning? Investigations with linear models: https://arxiv.org/abs/2211.15661:


## LLMs doing Math:
+ https://huggingface.co/papers/2309.03241

## Optimal Transort
+ https://www.youtube.com/watch?v=EauDdCzxphE

## Geometry and Topology in Machine Learning
+ https://www.youtube.com/watch?v=sS8zRGgPLNc
+ Meaning comes from geometry most of the time.


## Dead neurons Question/ Mystery
+ https://lena-voita.github.io/posts/neurons_in_llms_dead_ngram_positional.html 
   + https://arxiv.org/pdf/2309.04827.pdf
## Physic models in DL
+ https://www.youtube.com/watch?v=XLNmgviQHPA LEGO paper
+ 
