LLM tasks taxonomy: https://arxiv.org/pdf/2307.10169.pdf
QFT: resoruce https://www.youtube.com/watch?v=liQoSIaYBJk&list=PLXS_wzQAoIt7GxSRwCaicaSdeKqzYSUq4

## Physics Lens
+ Physics-Informed Machine Learning: A Survey
on Problems, Methods and Applications : Survey
+ Equivariant Flows: Exact Likelihood Generative Learning for Symmetric
Densities Particles here!!
+ A mathematical perspective on Transformers. Describes Transformers as Transformers based on their interpretation as interacting particle systems
 ### Quantum Field Theory and NNS
 + Neural Networks and Quantum Field Theory : https://arxiv.org/abs/2008.08601 
 + Neural Network Field Theories:
Non-Gaussianity, Actions, and Locality : https://arxiv.org/pdf/2307.03223.pdf
+ Symmetry-via-Duality:Invariant Neural Network Densities from Parameter-Space Correlators.
  + Discusses duality and symetry approach in qft. Can be used in NTK.  
+ Random Matrix Theory and Quantum Chromodynamics

    Gauge equavariant (Flow models) <----Symetry duality  <--QFT == NN
                                               â†“
 Symetry duality ---> lattice  QCD ( Quantum chromodynamics) ---------> Random Matrix Theory ----->> Neural Tangent Kernel 
                                                                                                        
                                               
---
## Random Matrix Theory
+ https://arxiv.org/abs/1812.03403 | Statistical thresholds for Tensor PCA
+ A Random Matrix Perspective on Random Tensors: https://www.jmlr.org/papers/volume23/21-1038/21-1038.pdf
+ MATRICES WITH GAUSSIAN NOISE: OPTIMAL ESTIMATES FOR SINGULAR SUBSPACE PERTURBATION
+ Two-way kernel matrix puncturing:
+  !!! Covariance discriminative power of kernel clustering methods **IMPORTANT FOR UNDERSTANDING SPECTRAL CLUSTERING METHODS**
  + Its to better understand performance of the used kernal using rmt.  
+ PCA-based multi-task learning: a random matrix approach
+ A CONCENTRATION OF MEASURE AND RANDOM MATRIX APPROACH TO LARGE-DIMENSIONAL ROBUST STATISTICS
+ A RANDOM MATRIX APPROACH TO NEURAL NETWORKS
+ Two-way kernel matrix puncturing:
towards resource-efficient PCA and spectral clustering
+ KERNEL SPECTRAL CLUSTERING OF LARGE DIMENSIONAL DATA
  + Its about theorotical base of the kernal spectral clustering.
+ Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars
#### MNIST related in RMT
  + A Large Dimensional Analysis of Least Squares **Support Vector Machines**
  + Random Matrix Improved **Subspace Clustering**
  + Sparse Quantized **Spectral Clustering**
  + **SEMI SUPERVISED LAERNING**  Random Matrix Analysis to Balance between Supervised
and Unsupervised Learning under the Low Density Separation Assumptioncan be used in mnist-1d again
### Understanding NNs via Random matrix theory:
+ DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES
### Theory on Universiality  of GMM
+ Classification of Heavy-tailed Features in High Dimensions: a Superstatistical Approach
#### Footprint improvement 
+ RANDOM MATRICES IN SERVICE OF ML FOOTPRINT:
TERNARY RANDOM FEATURES WITH NO PERFORMANCE LOSS

---- 
Interpretability Claims:
+ https://arxiv.org/abs/2312.01429 Interpretability methods might be misleading.

---

## Information Theory
- Grokking as Compression: A Nonlinear Complexity Perspective  https://arxiv.org/pdf/2310.05918.pdf 
-  In search of dispersed memories: Generative diffusion models are associative memory networks
https://arxiv.org/abs/2309.17290
- White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?
  https://arxiv.org/pdf/2311.13110.pdf
- White-Box Transformers via Sparse Rate Reduction shorter version : https://arxiv.org/pdf/2306.01129.pdf  
---
## Equaivariant NNs
+ https://dmol.pub/dl/Equivariant.html
### Harmonic Cnn like stuff
---
## Category Theory
+ Physical Computing:
A Category Theoretic Perspective on Physical
---
Computation and System Compositionality
## Anthropic Mechanistic Interpretability
+ https://transformer-circuits.pub/2023/monosemantic-features/index.html

## Untagged LLM
+ LANGUAGE MODELS REPRESENT SPACE AND TIME
 https://arxiv.org/pdf/2310.02207.pdf
+ Beyond Surface Statistics : https://arxiv.org/pdf/2306.05720.pdf
+ https://arxiv.org/abs/2308.09124:Linearity of Relation Decoding in Transformer Language Models
+ https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/iGuwZTHWb6DFY3sKB :Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron
+ What does self-attention learn from Masked Language Modelling? | https://arxiv.org/pdf/2304.07235.pdf
+ https://arxiv.org/pdf/2302.10886v3.pdf | SOME INTRIGUING ASPECTS ABOUT LIPSCHITZ CONTINUITY OF NEURAL NETWORKS
## Category Theory
 + https://arxiv.org/pdf/2106.07032.pdf Category Theory in Machine Learning

## In Context Learning Mystery
+ In-Context Learning Creates Task Vectors: https://arxiv.org/pdf/2310.15916.pdf
+ What learning algorithm is in-context learning? Investigations with linear models: https://arxiv.org/abs/2211.15661:


## LLMs doing Math:
+ https://huggingface.co/papers/2309.03241

## Optimal Transort
+ https://www.youtube.com/watch?v=EauDdCzxphE
+ https://openreview.net/pdf?id=KiespDPaRH
  + IMPROVING THE CONVERGENCE OF DYNAMIC NERFS VIA OPTIMAL TRANSPORT
## Geometry and Topology in Machine Learning
+ https://www.youtube.com/watch?v=sS8zRGgPLNc
+ Meaning comes from geometry most of the time.


## Dead neurons Question/ Mystery
+ https://lena-voita.github.io/posts/neurons_in_llms_dead_ngram_positional.html 
   + https://arxiv.org/pdf/2309.04827.pdf
## Physic models in DL
+ https://www.youtube.com/watch?v=XLNmgviQHPA LEGO paper
+ 
## Physics Methods in ML
+ # Main ResearchLine Interests
## Transformers are X papers from lens of Category Theory
+ Transformers are Cnn :https://huggingface.co/papers/2401.17574 | Scavenging Hyena: Distilling Transformers into Long Convolution Models | 
+ transformers are rnns papers
+ Transformers are graph attention models| https://thegradient.pub/transformers-are-graph-neural-networks/
+ LLM s are compression https://arxiv.org/pdf/2309.10668.pdf
+  A mathematical perspective on Transformers, Says transformers are flow models.!!! 
